# Dataset-Specific Watermarking

This repository contains the source code of the paper "I've Got Proof! Dataset-Specific Watermarking for Detecting Excessive Dataset Usage in Text-to-Image Diffusion Model Fine-Tuning". The proposed approach is "DSW: Dataset-Specific Watermarking".

# Requirement

- torch==1.12.0
- numpy==1.21.0
- torchvision==0.13.0

# Dataset

The experiments are evaluated on three datasets:

- pokemon-blip-captions  can be downloaded from [here](https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions).
- simpsons-blip-captions  can be downloaded from [here](https://huggingface.co/datasets/Norod78/simpsons-blip-captions).
- FaceCaption-1M-image-text-pairs can be downloaded from [here](https://huggingface.co/datasets/OpenFace-CQUPT/FaceCaption-1M-image-text-pairs/viewer).

Below we use **`pokemon-blip-captions`** as an example to illustrate how downloaded data and related files should be organized.

### 1. Training and Testing Data

Place the raw dataset used for model training and evaluation under the following paths:

```
dataset/pokemon-blip-captions/training_data/train/
dataset/pokemon-blip-captions/training_data/test/
```

- `train/`: data used for training the model
- `test/`: data used for evaluation or testing

### 2. LoRA Models Trained on Clean Data

LoRA models fine-tuned on the **clean dataset** should be stored in:

```
model/pokemon/train/clean/30epoch/
```

This directory contains the model checkpoints obtained after training for 30 epochs using clean data.

### 3. Data Generated by Clean LoRA Models

Data generated using the LoRA model trained on the clean dataset should be organized as follows:

```
dataset/pokemon-blip-captions/lora/clean/random/
dataset/pokemon-blip-captions/lora/train/
dataset/pokemon-blip-captions/lora/test/
```

- `lora/clean/random/`: randomly generated data using the clean LoRA model
- `lora/train/`: generated data used for training-related experiments
- `lora/test/`: generated data used for testing or evaluation

# Stable Diffusion 

The method of fine-tuning the Stable Diffusion model using LoRA can be found [here](https://github.com/justinpinkney/stable-diffusion).

```
python lora.py 
```

The Stable Diffusion codebase should be placed in the following directory:

```
stable-diffusion/
```

# Experiment

### Train the encoder and decoder

```
python train.py 
```

### Pre-trained Model

The pre-trained encoders and decoders can be found [here](https://drive.google.com/drive/folders/1JQdB_8sgdI5IbdCJFHyyjBLh-IAuk2vo?usp=drive_link).
